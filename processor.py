from llama_cpp import Llama
import os

# Ruta al modelo
MODEL_PATH = "./models/mistral/mistral-7b-instruct-v0.1.Q4_K_M.gguf"

# Inicializar el modelo
llm = Llama(model_path=MODEL_PATH, n_ctx=2048)

# Leer la transcripci√≥n
with open("output/transcripcion.txt", "r", encoding="utf-8") as f:
    transcripcion = f.read()

# Prompt para estructurar el resumen
prompt = f"""
Eres un asistente experto en gesti√≥n de proyectos de IT. Has recibido la siguiente transcripci√≥n de una reuni√≥n laboral. Quiero que generes un resumen estructurado con los siguientes apartados:

1. üìå **Resumen general** de los temas tratados.
2. ‚úÖ **Lista de tareas o decisiones**, indicando:
   - Tarea o acci√≥n
   - Persona/s responsable/s
   - Fecha mencionada o estimada
   - Deadline (si se infiere)
   - Detalles relevantes
3. üõ†Ô∏è **Sugerencias de correcci√≥n de errores ortogr√°ficos o confusiones comunes**, basadas en jerga t√©cnica (por ejemplo: 'CUA' podr√≠a ser 'QA', 'power vi' deber√≠a ser 'Power BI').

Aqu√≠ est√° la transcripci√≥n:
\"\"\"
{transcripcion}
\"\"\"

Responde en formato claro y estructurado, usando listas y markdown si lo deseas.
"""

# Ejecutar el modelo
respuesta = llm(prompt=prompt, max_tokens=1024, stop=["</s>"])

# Grabar la respuesta generada
with open("output/resumen_reunion.md", "w", encoding="utf-8") as f:
    f.write(respuesta["choices"][0]["text"].strip())

